{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13452811,"sourceType":"datasetVersion","datasetId":8539258}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport cv2\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np \nimport matplotlib.pyplot as plt\n\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom joblib import Parallel, delayed\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.preprocessing import StandardScaler\n\nfrom skimage import io, color\nfrom skimage.feature import hog\nfrom skimage.transform import resize","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-24T06:26:32.396242Z","iopub.execute_input":"2025-10-24T06:26:32.396597Z","iopub.status.idle":"2025-10-24T06:26:32.404369Z","shell.execute_reply.started":"2025-10-24T06:26:32.396573Z","shell.execute_reply":"2025-10-24T06:26:32.403160Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"# Data loading","metadata":{}},{"cell_type":"code","source":"images_dir = '/kaggle/input/nhapmoncv/data/images'\n\nclasses = [d for d in os.listdir(images_dir) if os.path.isdir(os.path.join(images_dir, d))]\n\nlabel_map = {cls: idx for idx, cls in enumerate(classes)}\n\ndata = []\nfor cls in classes:\n    cls_folder = os.path.join(images_dir, cls)\n    for fname in os.listdir(cls_folder):\n        if fname.lower().endswith(('.jpg', '.jpeg', '.png')):\n            file_path = os.path.join(cls_folder, fname)\n            label = label_map[cls]\n            data.append((file_path, label))\n\nclasses = [d.split(\"-\")[-1] for d in os.listdir(images_dir) if os.path.isdir(os.path.join(images_dir, d))]\n\nlabel_map = {cls: idx for idx, cls in enumerate(classes)}\n\n\ndf = pd.DataFrame(data, columns=['filepath', 'label'])\nprint(df.head())\nprint(\"Number of images:\", len(df))\nprint(\"Number of classes:\", len(classes))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T06:15:10.064356Z","iopub.execute_input":"2025-10-24T06:15:10.065107Z","iopub.status.idle":"2025-10-24T06:15:11.954277Z","shell.execute_reply.started":"2025-10-24T06:15:10.065073Z","shell.execute_reply":"2025-10-24T06:15:11.953072Z"}},"outputs":[{"name":"stdout","text":"                                            filepath  label\n0  /kaggle/input/nhapmoncv/data/images/n02091635-...      0\n1  /kaggle/input/nhapmoncv/data/images/n02091635-...      0\n2  /kaggle/input/nhapmoncv/data/images/n02091635-...      0\n3  /kaggle/input/nhapmoncv/data/images/n02091635-...      0\n4  /kaggle/input/nhapmoncv/data/images/n02091635-...      0\nNumber of images: 20580\nNumber of classes: 120\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"label_map = {v:k for k,v in label_map.items()}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T06:15:14.371559Z","iopub.execute_input":"2025-10-24T06:15:14.371893Z","iopub.status.idle":"2025-10-24T06:15:14.377018Z","shell.execute_reply.started":"2025-10-24T06:15:14.371865Z","shell.execute_reply":"2025-10-24T06:15:14.376076Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"df[\"breed\"] = df[\"label\"].map(label_map)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T06:15:15.647561Z","iopub.execute_input":"2025-10-24T06:15:15.647918Z","iopub.status.idle":"2025-10-24T06:15:15.668512Z","shell.execute_reply.started":"2025-10-24T06:15:15.647894Z","shell.execute_reply":"2025-10-24T06:15:15.667483Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T06:15:20.132696Z","iopub.execute_input":"2025-10-24T06:15:20.133017Z","iopub.status.idle":"2025-10-24T06:15:20.164445Z","shell.execute_reply.started":"2025-10-24T06:15:20.132994Z","shell.execute_reply":"2025-10-24T06:15:20.163169Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                                                filepath  label       breed\n0      /kaggle/input/nhapmoncv/data/images/n02091635-...      0  otterhound\n1      /kaggle/input/nhapmoncv/data/images/n02091635-...      0  otterhound\n2      /kaggle/input/nhapmoncv/data/images/n02091635-...      0  otterhound\n3      /kaggle/input/nhapmoncv/data/images/n02091635-...      0  otterhound\n4      /kaggle/input/nhapmoncv/data/images/n02091635-...      0  otterhound\n...                                                  ...    ...         ...\n20575  /kaggle/input/nhapmoncv/data/images/n02088466-...    119  bloodhound\n20576  /kaggle/input/nhapmoncv/data/images/n02088466-...    119  bloodhound\n20577  /kaggle/input/nhapmoncv/data/images/n02088466-...    119  bloodhound\n20578  /kaggle/input/nhapmoncv/data/images/n02088466-...    119  bloodhound\n20579  /kaggle/input/nhapmoncv/data/images/n02088466-...    119  bloodhound\n\n[20580 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filepath</th>\n      <th>label</th>\n      <th>breed</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/input/nhapmoncv/data/images/n02091635-...</td>\n      <td>0</td>\n      <td>otterhound</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/input/nhapmoncv/data/images/n02091635-...</td>\n      <td>0</td>\n      <td>otterhound</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/input/nhapmoncv/data/images/n02091635-...</td>\n      <td>0</td>\n      <td>otterhound</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/input/nhapmoncv/data/images/n02091635-...</td>\n      <td>0</td>\n      <td>otterhound</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/kaggle/input/nhapmoncv/data/images/n02091635-...</td>\n      <td>0</td>\n      <td>otterhound</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>20575</th>\n      <td>/kaggle/input/nhapmoncv/data/images/n02088466-...</td>\n      <td>119</td>\n      <td>bloodhound</td>\n    </tr>\n    <tr>\n      <th>20576</th>\n      <td>/kaggle/input/nhapmoncv/data/images/n02088466-...</td>\n      <td>119</td>\n      <td>bloodhound</td>\n    </tr>\n    <tr>\n      <th>20577</th>\n      <td>/kaggle/input/nhapmoncv/data/images/n02088466-...</td>\n      <td>119</td>\n      <td>bloodhound</td>\n    </tr>\n    <tr>\n      <th>20578</th>\n      <td>/kaggle/input/nhapmoncv/data/images/n02088466-...</td>\n      <td>119</td>\n      <td>bloodhound</td>\n    </tr>\n    <tr>\n      <th>20579</th>\n      <td>/kaggle/input/nhapmoncv/data/images/n02088466-...</td>\n      <td>119</td>\n      <td>bloodhound</td>\n    </tr>\n  </tbody>\n</table>\n<p>20580 rows Ã— 3 columns</p>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"# Feature extraction using ORB","metadata":{}},{"cell_type":"code","source":"orb = cv2.ORB_create(nfeatures=500)\n\nall_descriptors = []\nfor img_path in list(df['filepath']):\n    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n    kp, des = orb.detectAndCompute(img, None)\n    if des is not None:\n        all_descriptors.extend(des)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T06:15:22.665897Z","iopub.execute_input":"2025-10-24T06:15:22.666213Z","iopub.status.idle":"2025-10-24T06:20:54.355484Z","shell.execute_reply.started":"2025-10-24T06:15:22.666189Z","shell.execute_reply":"2025-10-24T06:20:54.354324Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"k = 200  # number of visual words\nkmeans = MiniBatchKMeans(n_clusters=k).fit(all_descriptors)\n\n# Represent each image as histogram of visual words\ndef get_bovw_vector(des, kmeans):\n    hist = np.zeros(k)\n    if des is not None:\n        words = kmeans.predict(des)\n        for w in words:\n            hist[w] += 1\n    return hist","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T06:20:54.357035Z","iopub.execute_input":"2025-10-24T06:20:54.357323Z","iopub.status.idle":"2025-10-24T06:21:26.127332Z","shell.execute_reply.started":"2025-10-24T06:20:54.357302Z","shell.execute_reply":"2025-10-24T06:21:26.126570Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"X = [get_bovw_vector(orb.detectAndCompute(cv2.imread(p, 0), None)[1], kmeans) for p in list(df['filepath'])]\nX = StandardScaler().fit_transform(X)\n\ny = np.array(list(df[\"label\"]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T06:21:26.128009Z","iopub.execute_input":"2025-10-24T06:21:26.128237Z","iopub.status.idle":"2025-10-24T06:25:27.234062Z","shell.execute_reply.started":"2025-10-24T06:21:26.128220Z","shell.execute_reply":"2025-10-24T06:25:27.232928Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T06:25:27.237209Z","iopub.execute_input":"2025-10-24T06:25:27.237622Z","iopub.status.idle":"2025-10-24T06:25:27.254812Z","shell.execute_reply.started":"2025-10-24T06:25:27.237596Z","shell.execute_reply":"2025-10-24T06:25:27.253531Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"rf = RandomForestClassifier()\nrf.fit(X_train, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T06:25:27.255928Z","iopub.execute_input":"2025-10-24T06:25:27.256272Z","iopub.status.idle":"2025-10-24T06:26:00.016673Z","shell.execute_reply.started":"2025-10-24T06:25:27.256239Z","shell.execute_reply":"2025-10-24T06:26:00.015424Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"RandomForestClassifier()","text/html":"<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"y_pred = rf.predict(X_test)\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T06:26:00.018181Z","iopub.execute_input":"2025-10-24T06:26:00.018587Z","iopub.status.idle":"2025-10-24T06:26:00.505720Z","shell.execute_reply.started":"2025-10-24T06:26:00.018552Z","shell.execute_reply":"2025-10-24T06:26:00.504743Z"}},"outputs":[{"name":"stdout","text":"Accuracy: 0.03255587949465501\n","output_type":"stream"}],"execution_count":13}]}